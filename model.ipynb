{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports...\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats\n",
    "# visualize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('figure', figsize=(11, 9))\n",
    "plt.rc('font', size=13)\n",
    "# turn off pink warning boxes\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# acquire\n",
    "from env import host, user, password\n",
    "from pydataset import data\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# my fancy docs\n",
    "import acquire\n",
    "import prepare\n",
    "import explore\n",
    "import model_func\n",
    "import mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Using the titanic data, in your classification-exercises repository, create a notebook, model.ipynb where you will do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive: survives\n",
    "# negative: dies\n",
    "# TP: predict survival, and the passenger actually survived\n",
    "# TN: predict death, and passenger actually died\n",
    "# FN: predict death, but passenger actually survived\n",
    "# FP: predict survival, but passenger actually died"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((498, 11), (214, 11), (179, 11))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets bring in the data set\n",
    "# titanic_df = acquire.new_titanic_data()\n",
    "# now let's prep it\n",
    "train, validate, test = prepare.prep_titanic_data(acquire.new_titanic_data(), column = 'age', method = 'median', dummies = ['embarked', 'sex'])\n",
    "# split it\n",
    "# train, validate, test = prepare.titanic_split(titanic_df)\n",
    "# what do they look like?\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 498 entries, 583 to 744\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   passenger_id  498 non-null    int64  \n",
      " 1   survived      498 non-null    int64  \n",
      " 2   pclass        498 non-null    int64  \n",
      " 3   age           498 non-null    float64\n",
      " 4   sibsp         498 non-null    int64  \n",
      " 5   parch         498 non-null    int64  \n",
      " 6   fare          498 non-null    float64\n",
      " 7   alone         498 non-null    int64  \n",
      " 8   embarked_Q    498 non-null    uint8  \n",
      " 9   embarked_S    498 non-null    uint8  \n",
      " 10  sex_male      498 non-null    uint8  \n",
      "dtypes: float64(2), int64(6), uint8(3)\n",
      "memory usage: 36.5 KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train.drop(columns='survived'), train.survived\n",
    "X_validate, y_validate = validate.drop(columns='survived'), validate.survived\n",
    "X_test, y_test = test.drop(columns='survived'), test.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pclass\n",
      "\n",
      "chi^2 = 55.2252\n",
      "p     = 0.0000\n",
      "------------------------------------\n",
      "     sibsp\n",
      "\n",
      "chi^2 = 24.8926\n",
      "p     = 0.0004\n",
      "------------------------------------\n",
      "     parch\n",
      "\n",
      "chi^2 = 15.4412\n",
      "p     = 0.0086\n",
      "-------------------------------------\n",
      "     alone\n",
      "\n",
      "chi^2 = 18.1920\n",
      "p     = 0.0000\n",
      "-------------------------------------\n",
      "     embarked_Q\n",
      "\n",
      "chi^2 = 0.3542\n",
      "p     = 0.5517\n",
      "-------------------------------------\n",
      "     embarked_S\n",
      "\n",
      "chi^2 = 12.3251\n",
      "p     = 0.0004\n",
      "-------------------------------------\n",
      "     sex_male\n",
      "\n",
      "chi^2 = 159.2890\n",
      "p     = 0.0000\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# i want to see relationships between each column and 'survived'\n",
    "# 'pclass' - chi2\n",
    "observed1 = pd.crosstab(train.survived, train.pclass)\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed1)\n",
    "print('     pclass')\n",
    "print('')\n",
    "print(f'chi^2 = {chi2:.4f}')\n",
    "print(f'p     = {p:.4f}')\n",
    "print('------------------------------------')\n",
    "# 'sibsp' - chi2\n",
    "observed2 = pd.crosstab(train.survived, train.sibsp)\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed2)\n",
    "print('     sibsp')\n",
    "print('')\n",
    "print(f'chi^2 = {chi2:.4f}')\n",
    "print(f'p     = {p:.4f}')\n",
    "print('------------------------------------')\n",
    "# 'parch' - chi2\n",
    "observed3 = pd.crosstab(train.survived, train.parch)\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed3)\n",
    "print('     parch')\n",
    "print('')\n",
    "print(f'chi^2 = {chi2:.4f}')\n",
    "print(f'p     = {p:.4f}')\n",
    "print('-------------------------------------')\n",
    "# 'alone' - chi2\n",
    "observed4 = pd.crosstab(train.survived, train.alone)\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed4)\n",
    "print('     alone')\n",
    "print('')\n",
    "print(f'chi^2 = {chi2:.4f}')\n",
    "print(f'p     = {p:.4f}')\n",
    "print('-------------------------------------')\n",
    "# 'embarked_Q' - chi2\n",
    "observed5 = pd.crosstab(train.survived, train.embarked_Q)\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed5)\n",
    "print('     embarked_Q')\n",
    "print('')\n",
    "print(f'chi^2 = {chi2:.4f}')\n",
    "print(f'p     = {p:.4f}')\n",
    "print('-------------------------------------')\n",
    "# 'embarked_S' - chi2\n",
    "observed6 = pd.crosstab(train.survived, train.embarked_S)\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed6)\n",
    "print('     embarked_S')\n",
    "print('')\n",
    "print(f'chi^2 = {chi2:.4f}')\n",
    "print(f'p     = {p:.4f}')\n",
    "print('-------------------------------------')\n",
    "# 'sex_male' - chi2\n",
    "observed7 = pd.crosstab(train.survived, train.sex_male)\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed7)\n",
    "print('     sex_male')\n",
    "print('')\n",
    "print(f'chi^2 = {chi2:.4f}')\n",
    "print(f'p     = {p:.4f}')\n",
    "print('-------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     age\n",
      "\n",
      "tscore: 49.39\n",
      "p/2:    3.8085496775045816e-194\n",
      "alpha:  0.01\n",
      "-------------------------------------\n",
      "     fare\n",
      "\n",
      "tscore: 14.55\n",
      "p/2:    1.6753909571183626e-40\n",
      "alpha:  0.01\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# now the continuous...\n",
    "# 'age'\n",
    "# 'fare'\n",
    "\n",
    "# create survival_rate\n",
    "survival_rate = train['survived'].mean()\n",
    "# create age variable for ttest\n",
    "age = train['age']\n",
    "# ttest for age\n",
    "alpha = 0.01\n",
    "t, p = stats.ttest_1samp(age, survival_rate)\n",
    "print('     age')\n",
    "print('')\n",
    "print('tscore:', t.round(2))\n",
    "print('p/2:   ', p/2)\n",
    "print('alpha: ', alpha)\n",
    "print('-------------------------------------')\n",
    "# create fare variable for ttest\n",
    "fare = train['fare']\n",
    "# ttest for fare\n",
    "t, p = stats.ttest_1samp(fare, survival_rate)\n",
    "print('     fare')\n",
    "print('')\n",
    "print('tscore:', t.round(2))\n",
    "print('p/2:   ', p/2)\n",
    "print('alpha: ', alpha)\n",
    "print('-------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so which columns do I want to keep for my model?????\n",
    "# 'sex_male', 'pclass', 'age', 'sibsp'\n",
    "# and maybe 'alone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = ['sex_male', 'pclass', 'age', 'sibsp']\n",
    "y_col = 'survived'\n",
    "\n",
    "X_train, y_train = train[X_cols], train[y_col]\n",
    "X_validate, y_validate = validate[X_cols], validate[y_col]\n",
    "X_test, y_test = test[X_cols], test[y_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train is the DF w/o the 'survived' column\n",
    "# y_train is the 'survived' column as a Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is your baseline prediction? What is your baseline accuracy? remember: your baseline prediction for a classification problem is predicting the most prevelant class in the training dataset (the mode). When you make those predictions, what is your accuracy? This is your baseline accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the mode of 'survived'?\n",
    "y_train.value_counts()\n",
    "# death wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the object\n",
    "baseline = DummyClassifier(strategy='constant', constant=0)\n",
    "# 2. Fit the object\n",
    "baseline.fit(X_train, y_train)\n",
    "# how does it do on training data set?\n",
    "print('Baseline accuracy: %.4f' % baseline.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fit the decision tree classifier to your training sample and transform (i.e. make predictions on the training sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look w/ default hyperparameters\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how does the DT perform w/ default hyperparameters?\n",
    "print(f'training score: {tree.score(X_train, y_train):.2%}')\n",
    "print(f'validate score: {tree.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the tree look like?\n",
    "print(export_text(tree, feature_names=X_train.columns.tolist(), show_weights=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is garbage\n",
    "\n",
    "plot_tree(tree, filled=True, rounded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's dive in and take a close look once we add some hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1 = DecisionTreeClassifier(max_leaf_nodes=4)\n",
    "tree1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'training score: {tree1.score(X_train, y_train):.2%}')\n",
    "print(f'validate score: {tree1.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(tree1, filled=True, rounded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluate your in-sample results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Tree 1 training score: {tree1.score(X_train, y_train):.2%}')\n",
    "print(f'Tree 1 validate score: {tree1.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compute: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add the predictions to the df\n",
    "\n",
    "train['prediction'] = tree1.predict(X_train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "confusion_matrix(train.survived, train.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.prediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre1_df = pd.DataFrame([['TN', 'FP'],['FN', 'TP']], index=['actual death', 'actual survived'], columns=['pred death', 'pred survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carl Sagan went to outer space to find Madeleine's code...\n",
    "pre1_df + ' : ' + confusion_matrix(train.survived, train.prediction).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st 4 create the variables\n",
    "tp = 141\n",
    "tn = 261\n",
    "fp = 46\n",
    "fn = 50\n",
    "# these calculate the rates\n",
    "tpr = tp/(tp+fn)\n",
    "fpr = fp/(fp+tn)\n",
    "tnr = tn/(tn+fp)\n",
    "fnr = fn/(fn+tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "accuracy_1 = (train.survived == train.prediction).mean()\n",
    "# Precision\n",
    "subset = train[train.prediction == 1]\n",
    "precision_t1 = (subset.prediction == subset.survived).mean()\n",
    "# Recall\n",
    "subset = train[train.survived == 1]\n",
    "recall_t1 = (subset.prediction == subset.survived).mean()\n",
    "# f1-score\n",
    "\n",
    "\n",
    "pd.DataFrame(classification_report(train.survived, train.prediction, output_dict=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The overall Accuracy is {accuracy_1:.2%}')\n",
    "print(f'The True Positive rate is {tpr:.2%}')\n",
    "print(f'The False Positive rate is {fpr:.2%}')\n",
    "print(f'The True Negative rate is {tnr:.2%}')\n",
    "print(f'The False Negative rate is {fnr:.2%}')\n",
    "print(f'Precision for tree 1 is {precision_t1:.2%}')\n",
    "print(f'Recall for tree 1 is {recall_t1:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = classification_report(train.survived, train.prediction, target_names=['Survive', 'Death'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(train.survived, train.prediction)\n",
    "precision = precision_score(train.survived, train.prediction, pos_label=1)\n",
    "recall = recall_score(train.survived, train.prediction, pos_label=1)\n",
    "classification = classification_report(train.survived, train.prediction, output_dict=True)\n",
    "pd.DataFrame(classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run through steps 2-4 using a different max_depth value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2 = DecisionTreeClassifier(max_depth=4)\n",
    "tree2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'training score: {tree2.score(X_train, y_train):.2%}')\n",
    "print(f'validate score: {tree2.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['prediction_2'] = tree2.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre2_df = pd.DataFrame([['TN', 'FP'],['FN', 'TP']], index=['actual death', 'actual survived'], columns=['pred_2 death', 'pred_2 survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre2_df + ' : ' + confusion_matrix(train.survived, train.prediction_2).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 140\n",
    "tn = 281\n",
    "fp = 26\n",
    "fn = 51\n",
    "tpr = tp/(tp+fn)\n",
    "fpr = fp/(fp+tn)\n",
    "tnr = tn/(tn+fp)\n",
    "fnr = fn/(fn+tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "accuracy_2 = (train.survived == train.prediction_2).mean()\n",
    "# Precision\n",
    "subset = train[train.prediction_2 == 1]\n",
    "precision_t2 = (subset.prediction_2 == subset.survived).mean()\n",
    "# Recall\n",
    "subset = train[train.survived == 1]\n",
    "recall_t2 = (subset.prediction_2 == subset.survived).mean()\n",
    "# f1-score\n",
    "\n",
    "\n",
    "class_report = pd.DataFrame(classification_report(train.survived, train.prediction_2, output_dict=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = class_report.rename(index={'0': 'Died', '1': 'Survived'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The overall Accuracy of Tree 2 is {accuracy_2:.2%}')\n",
    "print(f'The True Positive rate of Tree 2 is {tpr:.2%}')\n",
    "print(f'The False Positive rate Tree 2 is {fpr:.2%}')\n",
    "print(f'The True Negative rate of Tree 2 is {tnr:.2%}')\n",
    "print(f'The False Negative rate of Tree 2 is {fnr:.2%}')\n",
    "print(f'Precision for tree 2 is {precision_t2:.2%}')\n",
    "print(f'Recall for tree 2 is {recall_t2:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Which model performs better on your in-sample data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2 performs better on my train set\n",
    "# it is (at least slightly) overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Which model performs best on your out-of-sample data, the validate set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2 also performs better on the validate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for later... Thanks Heather, and Parker, and Chad\n",
    "\n",
    "def run_metrics(model, data_set):\n",
    "    \"\"\"\n",
    "    This function takes in a model and ouputs metrics. \n",
    "    model = name of class model\n",
    "    data_set = train, validate, test (AS A STRING)\n",
    "    Will output the Precision Score, the classification report, and the confusion matrix\n",
    "    It is advisable to print the name of the model you're working with before hand for clarity\n",
    "    i.e. print('Metrics for Model 1 with Train data\\n')\n",
    "    \"\"\"\n",
    "    if data_set == 'train':\n",
    "        X = X_train\n",
    "        y = y_train\n",
    "        df = train\n",
    "    if data_set == 'validate':\n",
    "        X = X_validate\n",
    "        y = y_validate\n",
    "        df = validate\n",
    "    if data_set == 'test':\n",
    "        X = X_test\n",
    "        y = y_test\n",
    "        df = test\n",
    "    score = model.score(X, y)\n",
    "    matrix = confusion_matrix(y, model.predict(X))\n",
    "    tpr = matrix[1,1] / (matrix[1,1] + matrix[1,0])\n",
    "    fpr = matrix[0,1] / (matrix[0,1] + matrix[0,0])\n",
    "    tnr = matrix[0,0] / (matrix[0,0] + matrix[0,1])\n",
    "    fnr = matrix[1,0] / (matrix[1,1] + matrix[1,0])\n",
    "    print(f'{data_set} data set accuracy score: {score:.2%}')\n",
    "    class_report = classification_report(y, model.predict(X), zero_division=True)\n",
    "    print('-------------------------------')\n",
    "    print(f'classification report')\n",
    "    print(class_report)\n",
    "    print ('-------------------------------')\n",
    "    print('')\n",
    "    print('confusion matrix')\n",
    "    print(matrix)\n",
    "    print(' ')\n",
    "    print(f'{data_set} data set model metrics')\n",
    "    print('---------------------------------')\n",
    "    print(f'True positive rate for the model is {tpr:.2%}')\n",
    "    print(f'False positive rate for the model is  {fpr:.2%}')\n",
    "    print(f'True negative rate for the model is {tnr:.2%}')\n",
    "    print(f'False negative rate for the model is {fnr:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fit the Random Forest classifier to your training sample and transform (i.e. make predictions on the training sample) setting the random_state accordingly and setting min_samples_leaf = 1 and max_depth = 10.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1 = RandomForestClassifier(bootstrap=True, \n",
    "                            class_weight=None, \n",
    "                            criterion='gini',\n",
    "                            min_samples_leaf=1,\n",
    "                            n_estimators=100,\n",
    "                            max_depth=10, \n",
    "                            random_state=1221)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf1.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = rf1.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of random forest classifier on training set: {:.2f}'\n",
    "     .format(rf1.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics(rf1, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Evaluate your results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run through steps increasing your min_samples_leaf and decreasing your max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = RandomForestClassifier(bootstrap=True, \n",
    "                            class_weight=None, \n",
    "                            criterion='gini',\n",
    "                            min_samples_leaf=3,\n",
    "                            n_estimators=100,\n",
    "                            max_depth=8, \n",
    "                            random_state=1221)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2.fit(X_train, y_train)\n",
    "y_pred = rf1.predict(X_train)\n",
    "y_pred_proba = rf1.predict_proba(X_train)\n",
    "run_metrics(rf2, 'train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf3 = RandomForestClassifier(bootstrap=True, \n",
    "                            class_weight=None, \n",
    "                            criterion='gini',\n",
    "                            min_samples_leaf=5,\n",
    "                            n_estimators=100,\n",
    "                            max_depth=8, \n",
    "                            random_state=1221)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf3.fit(X_train, y_train)\n",
    "y_pred = rf3.predict(X_train)\n",
    "y_pred_proba = rf3.predict_proba(X_train)\n",
    "run_metrics(rf3, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf 1 train vs validate\n",
    "run_metrics(rf1, 'validate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics(rf2, 'validate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics(rf3, 'validate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf4 = RandomForestClassifier(bootstrap=True, \n",
    "                            class_weight=None, \n",
    "                            criterion='gini',\n",
    "                            min_samples_leaf=3,\n",
    "                            n_estimators=100,\n",
    "                            max_depth=6, \n",
    "                            random_state=1221)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf4.fit(X_train, y_train)\n",
    "y_pred = rf4.predict(X_train)\n",
    "y_pred_proba = rf4.predict_proba(X_train)\n",
    "run_metrics(rf4, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics(rf4, 'validate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Thoughts... After making a few models, which one has the best performance (or closest metrics) on both train and validate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 is way overfit\n",
    "# 91% accuracy on train, 79% on validate\n",
    "\n",
    "# model 2 is still too overfit\n",
    "# 87% accuracy on train, 81% on validate\n",
    "\n",
    "# model 3 is tsimilar to model 2\n",
    "# 86% accuracy on train, 79% on validate\n",
    "\n",
    "# model 4 is the best so far...\n",
    "# 86% accuracy on train, 81% on validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics(tree2, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics(tree1, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I like this function better...\n",
    "model_func.model_performs(X_train, y_train, tree1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fit a K-Nearest Neighbors classifier to your training sample and transform (i.e. make predictions on the training sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['pclass', 'sex_male', 'age']]\n",
    "X_validate = validate[['pclass', 'sex_male', 'age']]\n",
    "y_train = train.survived\n",
    "y_validate = validate.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_1.fit(X_train, y_train)\n",
    "knn_1.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Evaluate your results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func.model_performs(X_train, y_train, knn_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'training score: {knn_1.score(X_train, y_train):.2%}')\n",
    "print(f'validate score: {knn_1.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run through steps 2-4 setting k to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_10 = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_10.fit(X_train, y_train)\n",
    "knn_10.score(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func.model_performs(X_train, y_train, knn_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'training score: {knn_10.score(X_train, y_train):.2%}')\n",
    "print(f'validate score: {knn_10.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Run through setps 2-4 setting k to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_20 = KNeighborsClassifier(n_neighbors=20)\n",
    "knn_20.fit(X_train, y_train)\n",
    "knn_20.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func.model_performs(X_train, y_train, knn_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'training score: {knn_20.score(X_train, y_train):.2%}')\n",
    "print(f'validate score: {knn_20.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.model_performs(X_train, y_train, knn_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(2,40,2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    accuracy = knn.score(X_validate, y_validate)\n",
    "    print(f'{k:2d}: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(2,40,2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(f'Model: {k}')\n",
    "    print(f'Training score: {knn.score(X_train, y_train):.2%}')\n",
    "    print(f'Validate Score: {knn.score(X_validate, y_validate):.2%}')\n",
    "    print('----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have an overview, lets tighten it up...\n",
    "\n",
    "for k in range(5,15):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(f'Model: {k}')\n",
    "    print(f'Training score: {knn.score(X_train, y_train):.2%}')\n",
    "    print(f'Validate Score: {knn.score(X_validate, y_validate):.2%}')\n",
    "    print('----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The higher k, the better it performs on in-sample data (overfit)\n",
    "- up until k = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Which model performs best on our out-of-sample data from validate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I think k=11 performs the best...\n",
    "- on train= 78.92%\n",
    "- on validate= 78.50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_11 = KNeighborsClassifier(n_neighbors=11)\n",
    "knn_11.fit(X_train, y_train)\n",
    "knn_11.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func.model_performs(X_train, y_train, knn_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # what if I simplified...\n",
    "# X_train = train[['sex_male', 'pclass']]\n",
    "# X_validate = validate[['sex_male', 'pclass']]\n",
    "# y_train = train.survived\n",
    "# y_validate = validate.survived\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in range(2,40,2):\n",
    "#     knn = KNeighborsClassifier(n_neighbors=k)\n",
    "#     knn.fit(X_train, y_train)\n",
    "#     print(f'Model: {k}')\n",
    "#     print(f'Training score: {knn.score(X_train, y_train):.2%}')\n",
    "#     print(f'Validate Score: {knn.score(X_validate, y_validate):.2%}')\n",
    "#     print('----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.compare(knn_11, knn_20, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf.compare_train_validate(knn_11, X_train, y_train, X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logically Logistic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a model that includes age in addition to fare and pclass. Does this model perform better than your baseline?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['pclass', 'fare', 'age']]\n",
    "X_validate = validate[['pclass', 'fare', 'age']]\n",
    "y_train = train.survived\n",
    "y_validate = validate.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_1 = LogisticRegression(C=1, class_weight={0:61, 1:39}, random_state = 1221, intercept_scaling = 1, solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficient: \\n', logit_1.coef_)\n",
    "print('Intercept: \\n', logit_1.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logit_1.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logit_1.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of Logistic Regression classifier 1 on training set: {:.2%}'.format(logit_1.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Include sex in your model as well. Note that you'll need to encode or create a dummy variable of this feature before including it in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['pclass', 'fare', 'age', 'sex_male']]\n",
    "X_validate = validate[['pclass', 'fare', 'age', 'sex_male']]\n",
    "y_train = train.survived\n",
    "y_validate = validate.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_2 = LogisticRegression(C=1, class_weight={0:61, 1:39}, random_state = 1221, intercept_scaling = 1, solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficient: \\n', logit_2.coef_)\n",
    "print('Intercept: \\n', logit_2.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logit_2.predict(X_train)\n",
    "y_pred_proba = logit_2.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of Logistic Regression classifier 2 on training set: {:.2%}'.format(logit_2.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Try out other combinations of features and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['pclass', 'fare', 'age', 'sex_male']]\n",
    "X_validate = validate[['pclass', 'fare', 'age', 'sex_male']]\n",
    "y_train = train.survived\n",
    "y_validate = validate.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_3 = LogisticRegression(C=0.5, random_state = 1221, solver = 'liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficient: \\n', logit_3.coef_)\n",
    "print('Intercept: \\n', logit_3.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logit_3.predict(X_train)\n",
    "y_pred_proba = logit_3.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of Logistic Regression classifier 3 on training set: {:.2%}'.format(logit_3.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['pclass', 'fare', 'age', 'sex_male']]\n",
    "X_validate = validate[['pclass', 'fare', 'age', 'sex_male']]\n",
    "y_train = train.survived\n",
    "y_validate = validate.survived\n",
    "logit_4 = LogisticRegression(C=0.5, random_state = 1221, fit_intercept=True, intercept_scaling=1, solver = 'liblinear')\n",
    "logit_4.fit(X_train, y_train)\n",
    "y_pred = logit_4.predict(X_train)\n",
    "y_pred_proba = logit_4.predict_proba(X_train)\n",
    "print('Coefficient: \\n', logit_4.coef_)\n",
    "print('Intercept: \\n', logit_4.intercept_)\n",
    "print('Accuracy of Logistic Regression classifier 4 on training set: {:.2%}'.format(logit_4.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['pclass', 'fare', 'age', 'sex_male']]\n",
    "X_validate = validate[['pclass', 'fare', 'age', 'sex_male']]\n",
    "y_train = train.survived\n",
    "y_validate = validate.survived\n",
    "logit_5 = LogisticRegression(C=1, random_state = 1221, fit_intercept=True, intercept_scaling=1, solver = 'lbfgs')\n",
    "logit_5.fit(X_train, y_train)\n",
    "y_pred = logit_5.predict(X_train)\n",
    "y_pred_proba = logit_5.predict_proba(X_train)\n",
    "print('Coefficient: \\n', logit_5.coef_)\n",
    "print('Intercept: \\n', logit_5.intercept_)\n",
    "print('Accuracy of Logistic Regression classifier 4 on training set: {:.2%}'.format(logit_5.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['pclass', 'age', 'sex_male']]\n",
    "X_validate = validate[['pclass', 'age', 'sex_male']]\n",
    "y_train = train.survived\n",
    "y_validate = validate.survived\n",
    "logit_6 = LogisticRegression(C=1, random_state = 1221, fit_intercept=True, intercept_scaling=1, solver = 'lbfgs')\n",
    "logit_6.fit(X_train, y_train)\n",
    "y_pred = logit_6.predict(X_train)\n",
    "y_pred_proba = logit_6.predict_proba(X_train)\n",
    "print('Coefficient: \\n', logit_6.coef_)\n",
    "print('Intercept: \\n', logit_6.intercept_)\n",
    "print('Accuracy of Logistic Regression classifier 4 on training set: {:.2%}'.format(logit_6.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use you best 3 models to predict and evaluate on your validate sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are my top three models?\n",
    "# 5, 6, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - 80.72% on train, 78.04% on validate\n",
    "X_train = train[['pclass', 'fare', 'age', 'sex_male']]\n",
    "X_validate = validate[['pclass', 'fare', 'age', 'sex_male']]\n",
    "y_train = train.survived\n",
    "y_validate = validate.survived\n",
    "logit_5 = LogisticRegression(C=1, random_state = 1221, fit_intercept=True, intercept_scaling=1, solver = 'lbfgs')\n",
    "logit_5.fit(X_train, y_train)\n",
    "y_pred = logit_5.predict(X_validate)\n",
    "y_pred_proba = logit_5.predict_proba(X_validate)\n",
    "print('Coefficient: \\n', logit_5.coef_)\n",
    "print('Intercept: \\n', logit_5.intercept_)\n",
    "print('Accuracy of Logistic Regression classifier 4 on training set: {:.2%}'.format(logit_5.score(X_validate, y_validate)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['pclass', 'age', 'sex_male']]\n",
    "X_validate = validate[['pclass', 'age', 'sex_male']]\n",
    "y_train = train.survived\n",
    "y_validate = validate.survived\n",
    "logit_6 = LogisticRegression(C=1, random_state = 1221, fit_intercept=True, intercept_scaling=1, solver = 'lbfgs')\n",
    "logit_6.fit(X_train, y_train)\n",
    "y_pred = logit_6.predict(X_validate)\n",
    "y_pred_proba = logit_6.predict_proba(X_validate)\n",
    "print('Coefficient: \\n', logit_6.coef_)\n",
    "print('Intercept: \\n', logit_6.intercept_)\n",
    "print('Accuracy of Logistic Regression classifier 4 on training set: {:.2%}'.format(logit_6.score(X_validate, y_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "X_train = train[['pclass', 'fare', 'age', 'sex_male']]\n",
    "X_validate = validate[['pclass', 'fare', 'age', 'sex_male']]\n",
    "y_train = train.survived\n",
    "y_validate = validate.survived\n",
    "logit_4 = LogisticRegression(C=0.5, random_state = 1221, fit_intercept=True, intercept_scaling=1, solver = 'liblinear')\n",
    "logit_4.fit(X_train, y_train)\n",
    "y_pred = logit_4.predict(X_validate)\n",
    "y_pred_proba = logit_4.predict_proba(X_validate)\n",
    "print('Coefficient: \\n', logit_4.coef_)\n",
    "print('Intercept: \\n', logit_4.intercept_)\n",
    "print('Accuracy of Logistic Regression classifier 4 on training set: {:.2%}'.format(logit_4.score(X_validate, y_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Choose you best model from the validation performation, and evaluate it on the test dataset. How do the performance metrics compare to validate? to train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 made it this far...\n",
    "X_train = train[['pclass', 'fare', 'age', 'sex_male']]\n",
    "X_validate = validate[['pclass', 'fare', 'age', 'sex_male']]\n",
    "y_train = train.survived\n",
    "y_validate = validate.survived\n",
    "logit_5 = LogisticRegression(C=1, random_state = 1221, fit_intercept=True, intercept_scaling=1, solver = 'lbfgs')\n",
    "logit_5.fit(X_train, y_train)\n",
    "y_pred = logit_5.predict(X_test)\n",
    "y_pred_proba = logit_5.predict_proba(X_test)\n",
    "print('Coefficient: \\n', logit_5.coef_)\n",
    "print('Intercept: \\n', logit_5.intercept_)\n",
    "print('Accuracy of Logistic Regression classifier 5 on test set: {:.2%}'.format(logit_5.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holy Coyote!!!!! That was big time overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "X_train = train[['pclass', 'fare', 'age', 'sex_male']]\n",
    "X_validate = validate[['pclass', 'fare', 'age', 'sex_male']]\n",
    "y_train = train.survived\n",
    "y_validate = validate.survived\n",
    "logit_4 = LogisticRegression(C=1, random_state = 1221, fit_intercept=True, intercept_scaling=1, solver = 'liblinear')\n",
    "logit_4.fit(X_train, y_train)\n",
    "y_pred = logit_4.predict(X_test)\n",
    "y_pred_proba = logit_4.predict_proba(X_test)\n",
    "print('Coefficient: \\n', logit_4.coef_)\n",
    "print('Intercept: \\n', logit_4.intercept_)\n",
    "print('Accuracy of Logistic Regression classifier 4 on training set: {:.2%}'.format(logit_4.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd0b64057e63add2b45b1ffc7eab9b09c8889b419c878e2fdf0d08f837f0fc857a7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
